{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51873f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import difflib\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pretty_midi\n",
    "from music21 import chord, stream, note\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca036c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Functions CSV (R_Pretrain) â†’ progressions\n",
    "# -------------------------------\n",
    "def binvec_to_pcset(binvec):\n",
    "    return [i for i, v in enumerate(binvec) if v == 1]\n",
    "\n",
    "def parse_binvec(cell):\n",
    "    if isinstance(cell, str):\n",
    "        cell = cell.replace('[', '').replace(']', '')\n",
    "        return [int(x) for x in cell.split()]\n",
    "    return list(cell)\n",
    "\n",
    "def csv_to_chord_progressions(csv_path, chord_cols):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    all_progressions = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        prog = []\n",
    "        for col in chord_cols:\n",
    "            binvec = parse_binvec(row[col])\n",
    "            pcset = binvec_to_pcset(binvec)\n",
    "            if len(pcset) >= 1:\n",
    "                prog.append(pcset)\n",
    "        if len(prog) > 0:\n",
    "            all_progressions.append(prog)\n",
    "    return all_progressions\n",
    "\n",
    "def transpose_progression(prog, semitones):\n",
    "    return [[(n + semitones) % 12 for n in chord] for chord in prog]\n",
    "\n",
    "def chord_to_token(chord):\n",
    "    return '_'.join(map(str, sorted(chord)))\n",
    "\n",
    "def build_vocab(progs):\n",
    "    token_to_id = {'<pad>':0, '<sos>':1, '<eos>':2}\n",
    "    id_to_token = {0:'<pad>', 1:'<sos>', 2:'<eos>'}\n",
    "    current_id = 3\n",
    "    for prog in progs:\n",
    "        for chord in prog:\n",
    "            tok = chord_to_token(chord)\n",
    "            if tok not in token_to_id:\n",
    "                token_to_id[tok] = current_id\n",
    "                id_to_token[current_id] = tok\n",
    "                current_id += 1\n",
    "    return token_to_id, id_to_token\n",
    "\n",
    "def progression_to_token_seq(prog, token_to_id):\n",
    "    return [token_to_id[chord_to_token(ch)] for ch in prog]\n",
    "\n",
    "# -------------------------------\n",
    "# PRETRAIN from CSV (R_Pretrain)\n",
    "# -------------------------------\n",
    "CSV_PATH = \"R_Pretrain.csv\"\n",
    "CHORD_COLS = [\"Chord_1\",\"Chord_2\",\"Chord_3\",\"Chord_4\",\"Chord_5\"]\n",
    "\n",
    "X_csv_raw = csv_to_chord_progressions(CSV_PATH, CHORD_COLS)\n",
    "print(f\"Total progresiones CSV (pretrain): {len(X_csv_raw)}\")\n",
    "\n",
    "# Transpose augmentations\n",
    "X_pretrain_raw = []\n",
    "for prog in X_csv_raw:\n",
    "    for shift in range(12):\n",
    "        X_pretrain_raw.append(transpose_progression(prog, shift))\n",
    "\n",
    "# vocab\n",
    "token_to_id_pretrain, id_to_token_pretrain = build_vocab(X_pretrain_raw)\n",
    "\n",
    "\n",
    "X_pretrain = []\n",
    "Y_pretrain = []\n",
    "for prog in X_pretrain_raw:\n",
    "    token_seq = progression_to_token_seq(prog, token_to_id_pretrain)\n",
    "    X_pretrain.append(torch.tensor(token_seq, dtype=torch.long))\n",
    "    Y_pretrain.append(torch.tensor([1] + token_seq + [2], dtype=torch.long))\n",
    "\n",
    "# -------------------------------\n",
    "# TRAIN from MIDIs\n",
    "# -------------------------------\n",
    "# - TRAIN_DIR -> MIDIs X,Y\n",
    "# -------------------------------\n",
    "# Datasets ans loaders\n",
    "# -------------------------------\n",
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, X_data, Y_data):\n",
    "        self.X = X_data\n",
    "        self.Y = Y_data\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        X, Y = zip(*batch)\n",
    "        return pad_sequence(X, batch_first=True, padding_value=0), pad_sequence(Y, batch_first=True, padding_value=0)\n",
    "\n",
    "pretrain_dataset = PretrainDataset(X_pretrain, Y_pretrain)\n",
    "pretrain_loader = DataLoader(pretrain_dataset, batch_size=8, shuffle=True, collate_fn=PretrainDataset.collate_fn)\n",
    "\n",
    "print(f\"Pretrain dataset size: {len(pretrain_dataset)}\")\n",
    "for xb, yb in pretrain_loader:\n",
    "    print(\"Pretrain batch shapes - X:\", xb.shape, \"Y:\", yb.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa7690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Config ----------------\n",
    "USE_PRETRAIN_MIDI = False  # True for pretrain from CSV(R_Pretrain)\n",
    "#  USE_PRETRAIN_MIDI = True para generar pretrain desde MIDIs\n",
    "USE_AUGMENTATION = False\n",
    "PRETRAIN_DIR = \"PRE_TRAIN\"\n",
    "TRAIN_DIR = \"MIDI_TRAIN\"\n",
    "TOLERANCE = 0.5\n",
    "MIN_NOTES = 2\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# ---------------- MIDIs ----------------\n",
    "def midi_to_chords_train(midi_path, tolerance=TOLERANCE, min_notes=MIN_NOTES):\n",
    "    pm = pretty_midi.PrettyMIDI(midi_path)\n",
    "    all_notes = []\n",
    "    for instrument in pm.instruments:\n",
    "        if instrument.is_drum:\n",
    "            continue\n",
    "        all_notes.extend(instrument.notes)\n",
    "\n",
    "    notes_by_time = defaultdict(list)\n",
    "    for note in all_notes:\n",
    "        time_key = round(note.start / tolerance) * tolerance\n",
    "        notes_by_time[time_key].append(note.pitch)\n",
    "\n",
    "    chords_out = []\n",
    "    for time in sorted(notes_by_time.keys()):\n",
    "        note_group = notes_by_time[time]\n",
    "        if len(note_group) < min_notes:\n",
    "            continue\n",
    "        try:\n",
    "            m21_chord = chord.Chord(note_group)\n",
    "            pc_set = sorted(set(n % 12 for n in note_group))\n",
    "        except Exception:\n",
    "            pc_set = sorted(set(n % 12 for n in note_group))\n",
    "        chords_out.append(pc_set)\n",
    "    return chords_out\n",
    "\n",
    "# ---------------- Other ----------------\n",
    "def transpose_progression(prog, semitones):\n",
    "    return [[(n + semitones) % 12 for n in chord] for chord in prog]\n",
    "\n",
    "def chord_to_token(chord):\n",
    "    return '_'.join(map(str, sorted(chord)))\n",
    "\n",
    "def build_vocab(progs):\n",
    "    token_to_id = {'<pad>': 0, '<sos>': 1, '<eos>': 2}\n",
    "    id_to_token = {0: '<pad>', 1: '<sos>', 2: '<eos>'}\n",
    "    current_id = 3\n",
    "    for prog in progs:\n",
    "        for chord in prog:\n",
    "            tok = chord_to_token(chord)\n",
    "            if tok not in token_to_id:\n",
    "                token_to_id[tok] = current_id\n",
    "                id_to_token[current_id] = tok\n",
    "                current_id += 1\n",
    "    return token_to_id, id_to_token\n",
    "\n",
    "def progression_to_token_seq(prog, token_to_id):\n",
    "    return [token_to_id[chord_to_token(ch)] for ch in prog]\n",
    "\n",
    "# ---------------- Pretrain from CSV ----------------\n",
    "X_csv_raw = csv_to_chord_progressions(CSV_PATH, CHORD_COLS)\n",
    "\n",
    "# ---------------- Pretrain ----------------\n",
    "X_pretrain_raw = []\n",
    "\n",
    "if USE_PRETRAIN_MIDI:\n",
    "    pretrain_files = [f for f in os.listdir(PRETRAIN_DIR) if f.endswith(\".mid\")]\n",
    "    for file_name in pretrain_files:\n",
    "        path = os.path.join(PRETRAIN_DIR, file_name)\n",
    "        chords = midi_to_chords_train(path)\n",
    "        if USE_AUGMENTATION:\n",
    "            for shift in range(12):\n",
    "                X_pretrain_raw.append(transpose_progression(chords, shift))\n",
    "        else:\n",
    "            X_pretrain_raw.append(chords)\n",
    "else:\n",
    "    X_csv_raw = csv_to_chord_progressions(CSV_PATH, CHORD_COLS)\n",
    "    for prog in X_csv_raw:\n",
    "        if USE_AUGMENTATION:\n",
    "            for shift in range(12):\n",
    "                X_pretrain_raw.append(transpose_progression(prog, shift))\n",
    "        else:\n",
    "            X_pretrain_raw.append(prog)\n",
    "\n",
    "token_to_id_pretrain, id_to_token_pretrain = build_vocab(X_pretrain_raw)\n",
    "\n",
    "X_pretrain = []\n",
    "Y_pretrain = []\n",
    "for prog in X_pretrain_raw:\n",
    "    token_seq = progression_to_token_seq(prog, token_to_id_pretrain)\n",
    "    X_pretrain.append(torch.tensor(token_seq, dtype=torch.long))\n",
    "    Y_pretrain.append(torch.tensor([1] + token_seq + [2], dtype=torch.long))\n",
    "\n",
    "with open(\"token_to_id_pretrain.pkl\", \"wb\") as f:\n",
    "    pickle.dump(token_to_id_pretrain, f)\n",
    "with open(\"id_to_token_pretrain.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id_to_token_pretrain, f)\n",
    "\n",
    "\n",
    "# ---------------- Superv-Train from MIDIs ----------------\n",
    "X_train_files = []\n",
    "Y_train_files = []\n",
    "\n",
    "for file_name in os.listdir(TRAIN_DIR):\n",
    "    index = file_name.split(\"_\")[0]\n",
    "    if file_name.startswith(index + \"_1_pf_org\"):\n",
    "        X_train_files.append((int(index), os.path.join(TRAIN_DIR, file_name)))\n",
    "    elif file_name.startswith(index + \"_2_pf_rhm\"):\n",
    "        Y_train_files.append((int(index), os.path.join(TRAIN_DIR, file_name)))\n",
    "\n",
    "X_train_files.sort(key=lambda x: x[0])\n",
    "Y_train_files.sort(key=lambda x: x[0])\n",
    "\n",
    "X_train_paths = [x[1] for x in X_train_files]\n",
    "Y_train_paths = [y[1] for y in Y_train_files]\n",
    "\n",
    "X_train_raw = [midi_to_chords_train(path) for path in X_train_paths]\n",
    "Y_train_raw = [midi_to_chords_train(path) for path in Y_train_paths]\n",
    "\n",
    "X_train_aug = []\n",
    "Y_train_aug = []\n",
    "\n",
    "for x_prog, y_prog in zip(X_train_raw, Y_train_raw):\n",
    "    if USE_AUGMENTATION:\n",
    "        for shift in range(12):\n",
    "            X_train_aug.append(transpose_progression(x_prog, shift))\n",
    "            Y_train_aug.append(transpose_progression(y_prog, shift))\n",
    "    else:\n",
    "        X_train_aug.append(x_prog)\n",
    "        Y_train_aug.append(y_prog)\n",
    "\n",
    "token_to_id_train, id_to_token_train = build_vocab(X_train_aug + Y_train_aug)\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for x_prog, y_prog in zip(X_train_aug, Y_train_aug):\n",
    "    x_tokens = progression_to_token_seq(x_prog, token_to_id_train)\n",
    "    y_tokens = progression_to_token_seq(y_prog, token_to_id_train)\n",
    "    X_train.append(torch.tensor(x_tokens, dtype=torch.long))\n",
    "    Y_train.append(torch.tensor([1] + y_tokens + [2], dtype=torch.long))\n",
    "\n",
    "# ---------------- Datasets ----------------\n",
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, X_data, Y_data):\n",
    "        self.X = X_data\n",
    "        self.Y = Y_data\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        X, Y = zip(*batch)\n",
    "        return pad_sequence(X, batch_first=True, padding_value=0), pad_sequence(Y, batch_first=True, padding_value=0)\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, X_data, Y_data):\n",
    "        self.X = X_data\n",
    "        self.Y = Y_data\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        X, Y = zip(*batch)\n",
    "        return pad_sequence(X, batch_first=True, padding_value=0), pad_sequence(Y, batch_first=True, padding_value=0)\n",
    "\n",
    "pretrain_dataset = PretrainDataset(X_pretrain, Y_pretrain)\n",
    "train_dataset = TrainDataset(X_train, Y_train)\n",
    "\n",
    "pretrain_loader = DataLoader(pretrain_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=PretrainDataset.collate_fn)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=TrainDataset.collate_fn)\n",
    "\n",
    "print(f\"Pretrain dataset size: {len(pretrain_dataset)}\")\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "\n",
    "# ---------------- Examples ----------------\n",
    "ej_idx = 0\n",
    "x_tokens, y_tokens = pretrain_dataset[ej_idx]\n",
    "x_chords = [id_to_token_pretrain.get(tok, \"?\") for tok in x_tokens.tolist()]\n",
    "y_chords = [id_to_token_pretrain.get(tok, \"?\") for tok in y_tokens.tolist()]\n",
    "\n",
    "print(\"ðŸ”¹  X (input):\", x_chords)\n",
    "print(\"ðŸ”¸  Y (target):\", y_chords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chord_to_token(chord):\n",
    "    return '_'.join(map(str, sorted(chord)))\n",
    "\n",
    "def fallback_token_str(chord_str, token_to_id):\n",
    "    matches = difflib.get_close_matches(chord_str, token_to_id.keys(), n=1)\n",
    "    return token_to_id[matches[0]] if matches else token_to_id.get('<pad>', 0)\n",
    "\n",
    "def progression_to_token_seq(prog, token_to_id):\n",
    "    tokens = []\n",
    "    for chord in prog:\n",
    "        token = chord_to_token(chord)\n",
    "        if token in token_to_id:\n",
    "            tokens.append(token_to_id[token])\n",
    "        else:\n",
    "            tokens.append(fallback_token_str(token, token_to_id))\n",
    "    return tokens\n",
    "\n",
    "class ChordPairDataset(Dataset):\n",
    "    def __init__(self, X_data, Y_data):\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_data[idx], self.Y_data[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        X_batch, Y_batch = zip(*batch)\n",
    "        X_padded = pad_sequence(X_batch, batch_first=True, padding_value=0)\n",
    "        Y_padded = pad_sequence(Y_batch, batch_first=True, padding_value=0)\n",
    "        return X_padded, Y_padded\n",
    "\n",
    "X_data = []\n",
    "Y_data = []\n",
    "\n",
    "for x_prog, y_prog in zip(X_train_aug, Y_train_aug): \n",
    "    x_tokens = progression_to_token_seq(x_prog, token_to_id_train)\n",
    "    y_tokens = [token_to_id_train['<sos>']] + progression_to_token_seq(y_prog, token_to_id_train) + [token_to_id_train['<eos>']]\n",
    "    X_data.append(torch.tensor(x_tokens, dtype=torch.long))\n",
    "    Y_data.append(torch.tensor(y_tokens, dtype=torch.long))\n",
    "\n",
    "X_train_split, X_val_split, Y_train_split, Y_val_split = train_test_split(\n",
    "    X_data, Y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = ChordPairDataset(X_train_split, Y_train_split)\n",
    "val_dataset = ChordPairDataset(X_val_split, Y_val_split)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=ChordPairDataset.collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=ChordPairDataset.collate_fn)\n",
    "\n",
    "for X_batch, Y_batch in train_loader:\n",
    "    print(\"X batch shape:\", X_batch.shape)\n",
    "    print(\"Y batch shape:\", Y_batch.shape)\n",
    "    break\n",
    "\n",
    "print(\"Total batches pretrain:\", len(pretrain_loader))\n",
    "print(\"Total batches train:\", len(train_loader))\n",
    "\n",
    "def token_seq_to_chords(token_seq, id_to_token):\n",
    "    chords = []\n",
    "    for idx in token_seq:\n",
    "        token = id_to_token.get(idx, '<unk>')\n",
    "        if token in ['<pad>', '<sos>', '<eos>']:\n",
    "            continue\n",
    "        chords.append(list(map(int, token.split('_'))))\n",
    "    return chords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e775588",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {'<pad>': 0, '<sos>': 1, '<eos>': 2}\n",
    "id_to_token = {0: '<pad>', 1: '<sos>', 2: '<eos>'}\n",
    "\n",
    "def chord_to_token(chord):\n",
    "    if isinstance(chord, torch.Tensor):\n",
    "        if chord.ndim == 0:\n",
    "            chord = [chord.item()]\n",
    "        else:\n",
    "            chord = chord.tolist()\n",
    "    return '_'.join(str(n) for n in sorted(chord))\n",
    "\n",
    "def construir_vocabulario(datasets):\n",
    "    for data in datasets:\n",
    "        for prog in data:\n",
    "            for chord in prog:\n",
    "                token = chord_to_token(chord)\n",
    "                if token not in token_to_id:\n",
    "                    idx = len(token_to_id)\n",
    "                    token_to_id[token] = idx\n",
    "                    id_to_token[idx] = token\n",
    "\n",
    "construir_vocabulario([X_train, Y_train, X_pretrain, Y_pretrain])\n",
    "vocab_size = len(token_to_id)\n",
    "\n",
    "print(f\"âœ… Vocab maked: {vocab_size} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946592f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, vocab_size, embedding_dim, num_layers=2, dropout=0.0):\n",
    "        super(CVAE_LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.decoder_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim * num_layers)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder_embedding(x)\n",
    "        _, (h_n, _) = self.encoder_lstm(x)\n",
    "        h_last = h_n[-1]\n",
    "        mu = self.fc_mu(h_last)\n",
    "        logvar = self.fc_logvar(h_last)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, y_seq, teacher_forcing_ratio=0.4):\n",
    "        batch_size, seq_len = y_seq.size()\n",
    "        embedding = self.embedding\n",
    "\n",
    "        hidden_state = torch.tanh(self.latent_to_hidden(z))\n",
    "        hidden = hidden_state.view(self.decoder_lstm.num_layers, batch_size, self.hidden_dim)\n",
    "        cell = torch.zeros_like(hidden).to(hidden.device)\n",
    "\n",
    "        inputs = y_seq[:, 0]\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(1, seq_len):\n",
    "            input_embed = embedding(inputs).unsqueeze(1)\n",
    "            output, (hidden, cell) = self.decoder_lstm(input_embed, (hidden, cell))\n",
    "            output_logits = self.output_layer(output.squeeze(1))\n",
    "            outputs.append(output_logits)\n",
    "\n",
    "            teacher_force = (torch.rand(1).item() < teacher_forcing_ratio)\n",
    "            top1 = output_logits.argmax(1)\n",
    "            inputs = y_seq[:, t] if teacher_force else top1\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def forward(self, x, y_seq=None, teacher_forcing_ratio=0.8):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        if y_seq is not None:\n",
    "            y_hat = self.decode(z, y_seq, teacher_forcing_ratio)\n",
    "            return y_hat, mu, logvar\n",
    "        else:\n",
    "            y_hat = self.generate(z, max_len=10)\n",
    "            return y_hat, mu, logvar\n",
    "    def generate(self, z, max_len=10, start_token_id=None, eos_token_id=None):\n",
    "        batch_size = z.size(0)\n",
    "        hidden_state = torch.tanh(self.latent_to_hidden(z))\n",
    "        hidden = hidden_state.view(self.decoder_lstm.num_layers, batch_size, self.hidden_dim)\n",
    "        cell = torch.zeros_like(hidden).to(hidden.device)\n",
    "\n",
    "        inputs = torch.full((batch_size,), start_token_id, dtype=torch.long).to(z.device)\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            input_embed = self.embedding(inputs).unsqueeze(1)\n",
    "            output, (hidden, cell) = self.decoder_lstm(input_embed, (hidden, cell))\n",
    "            output_logits = self.output_layer(output.squeeze(1))\n",
    "\n",
    "            probs = torch.softmax(output_logits, dim=-1)\n",
    "            inputs = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "\n",
    "            generated_tokens.append(inputs)\n",
    "\n",
    "            if eos_token_id is not None:\n",
    "                if (inputs == eos_token_id).all():\n",
    "                    break\n",
    "\n",
    "        generated_tokens = torch.stack(generated_tokens, dim=1)\n",
    "        return generated_tokens\n",
    "\n",
    "def detectar_escala(notas_midi):\n",
    "    s = stream.Stream()\n",
    "    for n in notas_midi:\n",
    "        s.append(note.Note(n))\n",
    "    k = s.analyze('key')\n",
    "    escala = k.getScale().getPitches()\n",
    "    escala_mod12 = sorted({p.pitchClass for p in escala})\n",
    "    return escala_mod12\n",
    "\n",
    "def tokens_to_root_notes(tokens):\n",
    "    root_notes = []\n",
    "    for token in tokens:\n",
    "        chord = id_to_token[token]\n",
    "        if chord in ['<sos>', '<eos>', '<pad>']:\n",
    "            continue\n",
    "        try:\n",
    "            notas = [int(x) for x in chord.split('_')]\n",
    "            root_note = min(notas)\n",
    "            root_notes.append(root_note)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return root_notes\n",
    "\n",
    "\n",
    "def loss_coherencia_tonal_soft(y_hat_step, root_note, escala_mod12):\n",
    "    probs = F.softmax(y_hat_step, dim=-1)\n",
    "    fuera_escala = torch.zeros(probs.size(-1), device=probs.device)\n",
    "    \n",
    "    for idx in range(probs.size(-1)):\n",
    "        chord = id_to_token[idx]\n",
    "        if chord in ['<sos>', '<eos>', '<pad>']:\n",
    "            fuera_escala[idx] = 0\n",
    "        else:\n",
    "            try:\n",
    "                notas = [root_note + int(x) for x in chord.split('_')]\n",
    "                count_fuera = sum(1 for n in notas if n % 12 not in escala_mod12)\n",
    "                fuera_escala[idx] = count_fuera / len(notas)\n",
    "            except:\n",
    "                fuera_escala[idx] = 0\n",
    "    loss = torch.sum(probs * fuera_escala)\n",
    "    return loss\n",
    "\n",
    "def loss_movimiento_armonico_soft(root_notes, device='cpu'):\n",
    "    loss = torch.tensor(0., device=device)\n",
    "    for i in range(1, len(root_notes)):\n",
    "        salto = abs(root_notes[i] - root_notes[i-1])\n",
    "        salto_tensor = torch.tensor(salto - 7.0, device=device)\n",
    "        loss += torch.relu(salto_tensor)\n",
    "    return loss / max(1, len(root_notes)-1)\n",
    "\n",
    "\n",
    "def loss_tensiones_soft(y_hat_step):\n",
    "    probs = F.softmax(y_hat_step, dim=-1)\n",
    "    penalizacion = torch.zeros(probs.size(-1), device=probs.device)\n",
    "\n",
    "    for idx in range(probs.size(-1)):\n",
    "        chord = id_to_token[idx]\n",
    "        if chord in ['<sos>', '<eos>', '<pad>']:\n",
    "            penalizacion[idx] = 0\n",
    "        else:\n",
    "            try:\n",
    "                num_notas = len(chord.split('_'))\n",
    "                penalizacion[idx] = 1 if num_notas < 4 else 0\n",
    "            except:\n",
    "                penalizacion[idx] = 0\n",
    "\n",
    "    loss = torch.sum(probs * penalizacion)\n",
    "    return loss\n",
    "\n",
    "def cvae_loss(y_hat, y, mu, logvar, beta=0.1, free_bits=1.0, pad_idx=0):\n",
    "    y_target = y[:, 1:]\n",
    "    recon_loss = F.cross_entropy(\n",
    "        y_hat.reshape(-1, y_hat.size(-1)),\n",
    "        y_target.reshape(-1),\n",
    "        reduction='sum',\n",
    "        ignore_index=pad_idx \n",
    "    )\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    kl_loss = torch.clamp(kl_loss, min=free_bits)\n",
    "    return recon_loss + beta * kl_loss, recon_loss, kl_loss\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CVAE_LSTM(\n",
    "    input_dim=32, \n",
    "    hidden_dim=64,\n",
    "    embedding_dim=64,\n",
    "    latent_dim=64,\n",
    "    num_layers=1, \n",
    "    dropout=0.0,\n",
    "    vocab_size=vocab_size,\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def entrenar_cvae(model, dataloader, optimizer, device, beta=0.1, teacher_forcing_ratio=0.8, usar_music_loss=False, w_coherencia=2.0, w_tensiones=0.1, w_movimiento=0.3):\n",
    "    model.train()\n",
    "    total_loss, total_recon, total_kl, total_music = 0, 0, 0, 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_hat, mu, logvar = model(x, y, teacher_forcing_ratio)\n",
    "        loss, recon, kl = cvae_loss(y_hat, y, mu, logvar, beta)\n",
    "\n",
    "        if usar_music_loss:\n",
    "            batch_music_loss = 0\n",
    "            for b in range(y.size(0)):\n",
    "                y_hat_seq = y_hat[b]\n",
    "                y_seq = y[b]\n",
    "                entrada = x[b].tolist()\n",
    "                root_note = None\n",
    "                for token_id in entrada:\n",
    "                    token = id_to_token[token_id]\n",
    "                    if token not in ['<sos>', '<pad>', '<eos>']:\n",
    "                        try:\n",
    "                            notas = [int(n) for n in token.split('_')]\n",
    "                            root_note = min(notas)\n",
    "                            break\n",
    "                        except:\n",
    "                            continue\n",
    "                if root_note is None:\n",
    "                    root_note = 0\n",
    "\n",
    "                escala = detectar_escala([root_note + int(n) for n in id_to_token[entrada[1]].split('_')])\n",
    "                for t in range(y_hat_seq.size(0)):\n",
    "                    y_hat_step = y_hat_seq[t]\n",
    "                    music1 = loss_coherencia_tonal_soft(y_hat_step, root_note, escala)\n",
    "                    music2 = loss_tensiones_soft(y_hat_step)\n",
    "                    batch_music_loss += w_coherencia * music1 + w_tensiones * music2\n",
    "\n",
    "                root_pred = tokens_to_root_notes(y_hat[b].argmax(dim=1).tolist())\n",
    "                batch_music_loss += w_movimiento * loss_movimiento_armonico_soft(root_pred, device=device)\n",
    "            batch_music_loss = batch_music_loss / (y.size(0) * y_hat.size(1))\n",
    "            loss += batch_music_loss\n",
    "            total_music += batch_music_loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_recon += recon.item()\n",
    "        total_kl += kl.item()\n",
    "\n",
    "    return total_loss, total_recon, total_kl, total_music\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10105988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_dataset(progresiones_x, progresiones_y=None, pad_value=0):\n",
    "    x_tensors = [torch.tensor(p, dtype=torch.long) for p in progresiones_x]\n",
    "    x_padded = pad_sequence(x_tensors, batch_first=True, padding_value=pad_value)\n",
    "\n",
    "    if progresiones_y is not None:\n",
    "        y_tensors = [torch.tensor(p, dtype=torch.long) for p in progresiones_y]\n",
    "        y_padded = pad_sequence(y_tensors, batch_first=True, padding_value=pad_value)\n",
    "        return TensorDataset(x_padded, y_padded)\n",
    "    else:\n",
    "        return TensorDataset(x_padded, x_padded)\n",
    "\n",
    "\n",
    "dataset_pre = preparar_dataset(X_pretrain)\n",
    "loader_pre = DataLoader(dataset_pre, batch_size=8, shuffle=True)\n",
    "\n",
    "print(\"== Fase 1: Preentrenamiento ==\")\n",
    "# 600\n",
    "for epoch in range(600):\n",
    "    loss, recon, kl, _ = entrenar_cvae(model, loader_pre, optimizer, device,\n",
    "                                       teacher_forcing_ratio=1, usar_music_loss=False)\n",
    "    print(f\"[{epoch+1}] Loss: {loss:.2f} | Recon: {recon:.2f} | KL: {kl:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549cfb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mostrar_reconstrucciones(model, dataset, id_to_token, device, token_to_id, num_muestras=5, max_len=5):\n",
    "    model.eval()\n",
    "    x_batch, _ = next(iter(DataLoader(dataset, batch_size=num_muestras, shuffle=True)))\n",
    "    x_batch = x_batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mu, logvar = model.encode(x_batch)\n",
    "        z = model.reparameterize(mu, logvar)\n",
    "        generated = model.generate(\n",
    "            z,\n",
    "            max_len=max_len,\n",
    "            start_token_id=token_to_id['<sos>'],\n",
    "            eos_token_id=token_to_id['<eos>']\n",
    "        )\n",
    "\n",
    "    for i in range(num_muestras):\n",
    "        entrada_tokens = x_batch[i].cpu().tolist()\n",
    "        salida_tokens = generated[i].cpu().tolist()\n",
    "\n",
    "        entrada_chords = [\n",
    "            id_to_token[tok]\n",
    "            for tok in entrada_tokens\n",
    "            if tok not in [token_to_id['<pad>'], token_to_id['<eos>'], token_to_id['<sos>']]\n",
    "        ]\n",
    "        salida_chords = [\n",
    "            id_to_token[tok]\n",
    "            for tok in salida_tokens\n",
    "            if tok not in [token_to_id['<pad>'], token_to_id['<eos>'], token_to_id['<sos>']]\n",
    "        ]\n",
    "\n",
    "        print(f\"\\nðŸ”¹ Input  ({i+1}): {' | '.join(entrada_chords)}\")\n",
    "        print(f\"ðŸ”¸ Reconstr ({i+1}): {' | '.join(salida_chords)}\")\n",
    "\n",
    "\n",
    "mostrar_reconstrucciones(model, dataset_pre, id_to_token_pretrain, device, token_to_id_pretrain, num_muestras=5, max_len=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d394d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = f\"scl_pretrained_{timestamp}.pt\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dafa60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = preparar_dataset(X_train, Y_train)\n",
    "loader_train = DataLoader(dataset_train, batch_size=8, shuffle=True)\n",
    "\n",
    "print(\"\\n== Fase 2: Train ==\")\n",
    "for epoch in range(100):\n",
    "    loss, recon, kl, music = entrenar_cvae(model, loader_train, optimizer, device,\n",
    "                                           teacher_forcing_ratio=0.9, usar_music_loss=True, w_coherencia=0.1, w_tensiones=2.0, w_movimiento=0.3)\n",
    "    print(f\"[{epoch+1}] Loss: {loss:.2f} | Recon: {recon:.2f} | KL: {kl:.2f} | Music: {music:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7fd525",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = f\"scl_fulltrained_{timestamp}.pt\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
